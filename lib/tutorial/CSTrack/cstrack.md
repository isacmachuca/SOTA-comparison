# CSTrack tutorial
## Testing

We assume the root path is $SOTS, e.g. `/home/zpzhang/SOTS`

### Set up environment

```
cd $TracKit/lib/tutorial
bash install.sh $conda_path TracKit
conda create -n CSTrack python=3.8
source activate CSTrack
cd SOTS/lib/tutorial/CSTrack/
pip install -r requirements.txt
```
`$conda_path` denotes your anaconda path, e.g. `/home/zpzhang/anaconda3`

### Prepare data and models
1. Download the pretrained model[Google driver](https://drive.google.com/drive/folders/1DfiuFP2xuclVLzPkPKYkMWJXHKAZLJmk?usp=sharing) to `$SOTS/weights`.
2. Download testing data e.g. MOT-16 and put them in `$SOTS/dataset`. The dataset can be downloaded from their [official webpage](https://motchallenge.net/).


### Prepare data and models
### Prepare dataset
We provide several relevant datasets for training and evaluating the CSTrack. 
Annotations are provided in a unified format. The datasets including Caltech, CityPersons, CUHK-SYSU, PRW, ETHZ and MOT-17 follow [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). The CrowdHuman dataset can be downloaded from their [official webpage](https://www.crowdhuman.org) or the following Baidu NetDisk and Google Drive we provide. 

All the datasets have the following structure:
```
Caltech
   |——————images
   |        └——————00001.jpg
   |        |—————— ...
   |        └——————0000N.jpg
   └——————labels_with_ids
            └——————00001.txt
            |—————— ...
            └——————0000N.txt
```
Every image has a corresponding annotation text. Given an image path, 
the annotation text path can be generated by replacing the string `images` with `labels_with_ids` and replacing `.jpg` with `.txt`.

In the annotation text, each line is describing a bounding box and has the following format:
```
[class] [identity] [x_center] [y_center] [width] [height]
```
The field `[class]` should be `0`. Only single-class multi-object tracking is supported in this version. 

The field `[identity]` is an integer from `0` to `num_identities - 1`, or `-1` if this box has no identity annotation.

***Note** that the values of `[x_center] [y_center] [width] [height]` are normalized by the width/height of the image, so they are floating point numbers ranging from 0 to 1.

#### Caltech Pedestrian
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1sYBXXvQaXZ8TuNwQxMcAgg)
[[1]](https://pan.baidu.com/s/1lVO7YBzagex1xlzqPksaPw) 
[[2]](https://pan.baidu.com/s/1PZXxxy_lrswaqTVg0GuHWg)
[[3]](https://pan.baidu.com/s/1M93NCo_E6naeYPpykmaNgA)
[[4]](https://pan.baidu.com/s/1ZXCdPNXfwbxQ4xCbVu5Dtw)
[[5]](https://pan.baidu.com/s/1kcZkh1tcEiBEJqnDtYuejg)
[[6]](https://pan.baidu.com/s/1sDjhtgdFrzR60KKxSjNb2A)
[[7]](https://pan.baidu.com/s/18Zvp_d33qj1pmutFDUbJyw)

Google Drive: [[annotations]](https://drive.google.com/file/d/1h8vxl_6tgi9QVYoer9XcY9YwNB32TE5k/view?usp=sharing) , 
please download all the images `.tar` files from [this page](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/datasets/USA/) and unzip the images under `Caltech/images`

You may need [this tool](https://github.com/mitmul/caltech-pedestrian-dataset-converter) to convert the original data format to jpeg images.
Original dataset webpage: [CaltechPedestrians](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
### CityPersons
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1g24doGOdkKqmbgbJf03vsw)
[[1]](https://pan.baidu.com/s/1mqDF9M5MdD3MGxSfe0ENsA) 
[[2]](https://pan.baidu.com/s/1Qrbh9lQUaEORCIlfI25wdA)
[[3]](https://pan.baidu.com/s/1lw7shaffBgARDuk8mkkHhw)

Google Drive:
[[0]](https://drive.google.com/file/d/1DgLHqEkQUOj63mCrS_0UGFEM9BG8sIZs/view?usp=sharing)
[[1]](https://drive.google.com/file/d/1BH9Xz59UImIGUdYwUR-cnP1g7Ton_LcZ/view?usp=sharing) 
[[2]](https://drive.google.com/file/d/1q_OltirP68YFvRWgYkBHLEFSUayjkKYE/view?usp=sharing)
[[3]](https://drive.google.com/file/d/1VSL0SFoQxPXnIdBamOZJzHrHJ1N2gsTW/view?usp=sharing)

Original dataset webpage: [Citypersons pedestrian detection dataset](https://bitbucket.org/shanshanzhang/citypersons)

### CUHK-SYSU
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1YFrlyB1WjcQmFW3Vt_sEaQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1D7VL43kIV9uJrdSCYl53j89RE2K-IoQA/view?usp=sharing)

Original dataset webpage: [CUHK-SYSU Person Search Dataset](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)

### PRW
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1iqOVKO57dL53OI1KOmWeGQ)

Google Drive:
[[0]](https://drive.google.com/file/d/116_mIdjgB-WJXGe8RYJDWxlFnc_4sqS8/view?usp=sharing)

Original dataset webpage: [Person Search in the Wild datset](http://www.liangzheng.com.cn/Project/project_prw.html)

### ETHZ (overlapping videos with MOT-16 removed):
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/14EauGb2nLrcB3GRSlQ4K9Q)

Google Drive:
[[0]](https://drive.google.com/file/d/19QyGOCqn8K_rc9TXJ8UwLSxCx17e0GoY/view?usp=sharing)

Original dataset webpage: [ETHZ pedestrian datset](https://data.vision.ee.ethz.ch/cvl/aess/dataset/)

### MOT-17
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1lHa6UagcosRBz-_Y308GvQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1ET-6w12yHNo8DKevOVgK1dBlYs739e_3/view?usp=sharing)

Original dataset webpage: [MOT-17](https://motchallenge.net/data/MOT17/)

### MOT-16 (for evaluation )
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/10pUuB32Hro-h-KUZv8duiw)

Google Drive:
[[0]](https://drive.google.com/file/d/1254q3ruzBzgn4LUejDVsCtT05SIEieQg/view?usp=sharing)

Original dataset webpage: [MOT-16](https://motchallenge.net/data/MOT16/)

### MOT-17
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1lHa6UagcosRBz-_Y308GvQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1ET-6w12yHNo8DKevOVgK1dBlYs739e_3/view?usp=sharing)

Original dataset webpage: [MOT-17](https://motchallenge.net/data/MOT17/)



### Testing

#### For VOT2020

1. Modify scripts
Set the model path in line81 of `$tracker_path/tracking/vot_wrap.py` and `$tracker_path/tracking/vot_wrap_mms.py`.

2. run

- for model without MMS network:
```
set running script in vot2020 workspace (i.e. trackers.ini) to `vot_wrap.py`
```
- for model with MMS network:
```
set running script in vot2020 workspace (i.e. trackers.ini) to `vot_wrap_mms.py`
```
- Note: We provided a reference of `trackers.ini` in `$tracker_path/trackers.ini`. Please find more running guidelines in VOT official [web](https://www.votchallenge.net/howto/tutorial_python.html).

#### For VOS (DAVIS/YTBVOS)
Coming soon ...

:cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud:

The training code will be released after accepted. Thanks for your interest!



#Train
python train.py --batch_size 10 --device 0 --data cfg/data_ch.json

#Test
python track.py --nms_thres 0.6
                         --conf_thres 0.5
                         --weights /CSTrack/runs/exp0_mot_test/weights/last_mot_test.pt
                         --device 0
                         --test_mot16 True
